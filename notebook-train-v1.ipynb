{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n>Just one more kernel with CNN in this competition :)\nTo work with large data, use ImageDataGenerator.flow_from_dataframe as input for model.\nFirst, import necessary libraries:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\n\n# Image processing\nimport imageio\nimport cv2\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ML, statistics\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n\n# Tensorflow\n#from sklearn.preprocessing import OneHotEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport time\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nINPUT_DIR = \"../input/plant-pathology-2021-fgvc8/\"\nPRETRAINED_DIR = \"../input/pretrained-model/\"\nOUTPUT_DIR =\"../working/\"\n#INPUT_DIR = \"../input/plant-pathology-2021-fgvc8/\"\n#PRETRAINED_DIR = \"../pretrained/\"\n#OUTPUT_DIR = \"./model/\"\n\n\nTRAIN_DATA_DIR = INPUT_DIR + 'train_images'\n#TRAIN_DATA_DIR = INPUT_DIR + 'img_sz_512'\n\n\nIMSIZES = (224, 240, 260, 300, 380, 456, 528, 600)\n\nNUM_CLASSES = 12\nIMAGE_SIZE= (224,224)  #(256,256)\n\nBATCH_SIZE = 16 #32  16\nTRAIN_BATCH_SIZE = 512 #512 256\n#16ï¼Œ512 works\n\nprint(os.listdir(OUTPUT_DIR))\nprint(os.listdir(INPUT_DIR))\n\ntoy=False\n\nall_df = pd.read_csv(INPUT_DIR + \"train.csv\")\nif toy:\n    all_df = all_df.sample(5000)\n\n\nNB_TRAIN = all_df.shape[0]  #15930 #4275 #32706\nNB_VALID = NB_TRAIN * 0.1 #1770 #475 #10902\n\nprint(all_df.shape)\nall_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\n>Check labels balancing\nCheck whether labels are balanced","metadata":{}},{"cell_type":"code","source":"all_df.labels.value_counts().plot(kind='bar')\nplt.title('Labels counts')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\nprint(all_df.labels.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageViewer:\n    def read_img(self,id, folder='train_images'):\n        \"\"\"\n        Read image by it's id\n        \"\"\"\n        file=INPUT_DIR + folder + '/' + str(id)\n        #print(file)\n        im=cv2.imread(file)\n        #print(im)\n        return im\n\n    def draw_sample_images(self):\n        \"\"\"\n        Draw disease and healthy images for EDA\n        \"\"\"\n        ncols=3\n        f, ax = plt.subplots(nrows=7,ncols=ncols, \n                             figsize=(5*ncols,5*4))\n        i=-1\n        #cider_apple_rust removed\n        captions=['healthy','scab','frog_eye_leaf_spot','rust','complex','powdery_mildew','scab frog_eye_leaf_spot']\n        # Draw one row for scab, one row for healthy images\n        for label in captions:\n            i=i+1\n            group = all_df[all_df['labels']==label]\n            samples = group['image'].sample(ncols).values\n            #print(samples)\n            for j in range(0,ncols):\n                file_id=samples[j]\n                im=self.read_img(file_id)\n                #print(im)\n                ax[i, j].imshow(im)\n                ax[i, j].set_title(captions[i], fontsize=16)  \n        plt.tight_layout()\n        plt.show()\n    \nImageViewer().draw_sample_images()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data preparation\n>Here we are going to balance dataset and prepare image generator\n\n## Train and test split","metadata":{}},{"cell_type":"code","source":"class DataPreparation:\n    \"\"\"\n    Train/test\n    \"\"\"\n    def train_test_split(self, all_df):\n        \"\"\"\n        Balanced split to train, test and val\n        \"\"\"\n        # Split to train and test before balancing\n        train_df, test_df = train_test_split(all_df, random_state=24, test_size=0.05, shuffle=False)\n        print(train_df.shape)\n#         # Split train to train and validation datasets\n#         train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=24)\n        # Number of samples in each category\n        \n        #ncat_bal = train_df['labels'].value_counts().max()\n#        #ncat_bal = int(len(train_df)/train_df['label'].cat.categories.size)\n        #train_df = train_df.groupby('labels', as_index=False).apply(lambda g:  g.sample(ncat_bal, replace=True, random_state=24)).reset_index(drop=True)\n        #print(train_df.shape, ncat_bal)\n        return train_df, test_df\n    \n    def plot_balanced(self, train_df, all_df):\n        \"\"\"\n        Plot samples per category before and after balancing\n        \"\"\"\n        f, axs = plt.subplots(1,2,figsize=(15,4))\n        # Before balancing\n        all_df.labels.value_counts().plot(kind='bar', ax=axs[0])\n        axs[0].set_title('All labels')\n        axs[0].set_xlabel('Label')\n        axs[0].set_ylabel('Count')\n        # After balancing\n        train_df.labels.value_counts().plot(kind='bar', ax=axs[1])\n        axs[1].set_title('Train labels after balancing')\n        axs[1].set_xlabel('Label')\n        axs[1].set_ylabel('Count')\n        plt.tight_layout()\n        plt.show()\n\n\n# Train/test/validation split with balanced labels in train\ndata_prep = DataPreparation()\ntrain_df, test_df = data_prep.train_test_split(all_df)\n\n# Plot before and after balancing\ndata_prep.plot_balanced(train_df, all_df)\nprint(all_df.shape,train_df.shape, test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create image generator\n>Keras ImageDataGenerator can work with dataframe of file names. Our train, validation and test dataframes contain file name in id column and ImageDataGenerator can understand id.","metadata":{}},{"cell_type":"code","source":"class Generators:\n    \"\"\"\n    Train, validation and test generators\n    \"\"\"\n    def __init__(self, train_df, test_df):\n        self.batch_size= BATCH_SIZE #16 #1024 #32 #8\n        self.img_size= IMAGE_SIZE #(256,256)#(512,512)#(192,128) #(96,64) (384,256)  #(4000,2672)\n\n        # Base train/validation generator\n        _datagen = ImageDataGenerator(\n            rescale=1./255.,\n            validation_split=0.1,\n            #featurewise_center=False,\n            #featurewise_std_normalization=False,\n            fill_mode='nearest',\n            zca_whitening=False,  # apply ZCA whitening\n            rotation_range=10, #180,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.15, # Randomly zoom image \n            width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip=True,  # randomly flip images\n            vertical_flip=True  # randomly flip images\n            )\n \n        # Train generator\n        self.train_generator = _datagen.flow_from_dataframe(\n            dataframe=train_df, #all_df, #\n            directory=TRAIN_DATA_DIR,\n            x_col=\"image\",\n            y_col=\"labels\",\n            #has_ext=False,\n            \n            subset=\"training\",\n            batch_size=self.batch_size,\n            seed=42,\n            shuffle=False, #True,\n            class_mode=\"categorical\",\n            target_size=self.img_size)\n        print('Train generator created')\n        # Validation generator\n        self.val_generator = _datagen.flow_from_dataframe(\n            dataframe=train_df,#test_df ,#\n            directory=TRAIN_DATA_DIR,\n            x_col=\"image\",\n            y_col=\"labels\",\n            #has_ext=False,\n            \n            subset=\"validation\",\n            batch_size=self.batch_size,\n            seed=42,\n            shuffle=False, #True,\n            class_mode=\"categorical\",\n            target_size=self.img_size)    \n        print('Validation generator created')\n        \n        \n        # Test generator\n        _test_datagen=ImageDataGenerator(rescale=1./255.)\n        self.test_generator = _test_datagen.flow_from_dataframe(\n            dataframe=test_df,\n            directory=TRAIN_DATA_DIR,\n            x_col=\"image\",\n            y_col='labels',\n            #has_ext=False,\n            class_mode=\"categorical\",\n            batch_size=self.batch_size,\n            seed=42,\n            shuffle=False,\n            target_size=self.img_size)     \n        print('Test generator created')\n\n        \n        \n        \n# Create generators   \n#(43608, 2) (4658, 2)\ngenerators = Generators(train_df, test_df)\nprint(\"Generators created\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(generators.train_generator.class_indices)\nclass_labels = generators.train_generator.class_indices\nclass_labels = {v: k for k, v in class_labels.items()}\nprint(class_labels)\nclasses = list(class_labels.values())\nprint(classes)        \n#print(len(self.generator.classes))\n\n#y_pred = []\n#for item in self.y_pred:\n#    y_pred.append(label_batch)\ndef get_class(one_hot):\n    for i in range(NUM_CLASSES):\n        if one_hot[i] == 1:\n            return classes[i]\n    \n            \n            \nw=10\nh=10\nfig=plt.figure(figsize=(20, 14))\ncolumns = 2\nrows = 4\nplt.title('Image Class')\nplt.axis('off')\nfor i in range(1, columns*rows):\n    \n    img_batch, label_batch = generators.train_generator.next()\n    fig.add_subplot(rows, columns, i)\n    \n    plt.title(get_class(label_batch[i]))  \n    plt.imshow(img_batch[i])\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Create and train the model\n>Put all creation and training code in one class. Will experiment with model architecture later.","metadata":{}},{"cell_type":"code","source":"LR_START = 0.00001\nLR_MAX = 0.0001 \nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 15\nLR_SUSTAIN_EPOCHS = 3\nLR_EXP_DECAY = .8\nEPOCHS = 100\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelTrainer:\n    \"\"\"\n    Create and fit the model\n    \"\"\"\n    \n    def __init__(self, generators):\n        #(18632, 2) (17700, 2) (932, 2)\n        #15930 , 1770, 932\n        \n        self.nb_train_samples = NB_TRAIN  #15930 #4275 #32706\n        self.nb_validation_samples = NB_VALID #1770 #475 #10902\n        self.batch_size =  TRAIN_BATCH_SIZE #512 #512 #32 #1024   #32\n               \n        \n        self.generators = generators\n        self.img_width = IMAGE_SIZE[0]\n        self.img_height = IMAGE_SIZE[1]\n        print(self.img_width,self.img_height)\n        \n    \n    def create_model(self, model_name):\n        #will preload and consume lots memory\n        #dict_model = {\n        #    \"Xception\": self.create_model_1(),\n        #    \"DenseNet121\": self.create_model_2(),\n        #    \"XceptionDenseNet121\": self.create_model_3(),\n        #    \"InceptionResNetV2\": self.create_model_4(),\n        #    \"ResNet50V2\": self.create_model_5(),\n        #    \"VGG16\": self.create_model_6(),\n        #    \"middle\": self.create_model_middle(),\n        #    \"small\": self.create_model_small(),\n        #}    \n        #print(\"Creating \"+model_name)\n        if model_name == \"Xception\" :\n                model = self.create_model_1()\n        elif model_name == \"EfficientNetB7\": \n                model = self.create_model_0()                \n        elif model_name == \"DenseNet121\": \n                model = self.create_model_2()\n        elif model_name == \"XceptionDenseNet121\": \n                model = self.create_model_3()\n        elif model_name == \"InceptionResNetV2\": \n                model = self.create_model_4()\n        elif model_name == \"ResNet50V2\": \n                model = self.create_model_5()\n        elif model_name == \"VGG16\": \n                model = self.create_model_6()\n        elif model_name == \"middle\": \n                model = self.create_model_middle()\n        elif model_name == \"small\": \n                model = self.create_model_small()\n        else:\n                print(\"Model not found\")\n        self.model_name = model_name\n        return model\n    \n\n    def create_model_0(self):\n        path0 = PRETRAINED_DIR + 'efficientnetb7_notop.h5'\n        pretrained_model = tf.keras.applications.EfficientNetB7(include_top=False, weights=path0, #drop_connect_rate=0.4,\n                                                                 input_shape=(self.img_width, self.img_height, 3))\n        model = tf.keras.models.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(NUM_CLASSES,activation='sigmoid')\n        ])\n        return model    \n    \n    #Total params: 20,886,068\n    def create_model_1(self):\n        path1 = PRETRAINED_DIR + 'xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        pretrained_model = tf.keras.applications.xception.Xception(include_top=False, weights=path1, input_shape=(self.img_width, self.img_height, 3))\n        model = tf.keras.models.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n        return model\n    \n    def create_model_2(self):\n        path2 = PRETRAINED_DIR + 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        pretrained_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights=path2,input_shape=(self.img_width, self.img_height, 3))\n        model = tf.keras.models.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n    \n        return model\n\n    def create_model_3(self):\n        path1 = PRETRAINED_DIR + 'xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        xception_model = tf.keras.models.Sequential([\n          tf.keras.applications.xception.Xception(include_top=False, weights=path1, input_shape=(self.img_width, self.img_height, 3)),#(512, 512, 3)),\n           tf.keras.layers.GlobalAveragePooling2D(),\n           tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n        #xception_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n        #xception_model.summary()\n        path2 = PRETRAINED_DIR + 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        densenet_model = tf.keras.models.Sequential([\n        tf.keras.applications.densenet.DenseNet121(include_top=False, weights=path2,input_shape=(self.img_width, self.img_height, 3)),#(512, 512, 3)),\n           tf.keras.layers.GlobalAveragePooling2D(),\n           tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n        #densenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n        #densenet_model.summary()\n        \n        inputs = tf.keras.Input(shape=(self.img_width, self.img_height, 3)) #(512, 512, 3))\n\n        xception_output = xception_model(inputs)\n        densenet_output = densenet_model(inputs)\n\n        outputs = tf.keras.layers.average([densenet_output, xception_output])\n\n\n        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n        return model\n \n    def create_model_4(self):\n        pretrained_model = tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(self.img_width, self.img_height, 3))\n        model = tf.keras.models.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n        return model\n    \n    def create_model_5(self):\n        pretrained_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_shape=(self.img_width, self.img_height, 3))\n        model = tf.keras.models.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n    \n        return model\n\n    #Total params: 14,720,844\n    def create_model_6(self):\n        pretrained_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(self.img_width, self.img_height, 3))\n        model = tf.keras.models.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n        ])\n        return model    \n    \n    def create_model_middle(self):\n        model = Sequential()\n\n        # Padding = 'same'  results in padding the input such that\n        # the output has the same length as the original input\n        model.add(Conv2D(32, (3, 3), padding='same',\n                         input_shape= (self.img_width, self.img_height, 3)))\n        model.add(Activation('relu'))\n        model.add(Conv2D(32, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Conv2D(64, (3, 3), padding='same'))\n        model.add(Activation('relu'))\n        model.add(Conv2D(64, (3, 3)))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n        model.add(Dense(12))\n        model.add(Activation('relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(NUM_CLASSES))\n        model.add(Activation('softmax'))\n\n        return model\n    \n    def create_model_vgg16(self):\n        input_shape = (224, 224, 3)\n\n        model = Sequential([\n            Conv2D(64, (3, 3), input_shape=input_shape, padding='same',\n                   activation='relu'),\n            Conv2D(64, (3, 3), activation='relu', padding='same'),\n            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n            Conv2D(128, (3, 3), activation='relu', padding='same'),\n            Conv2D(128, (3, 3), activation='relu', padding='same',),\n            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n            Conv2D(256, (3, 3), activation='relu', padding='same',),\n            Conv2D(256, (3, 3), activation='relu', padding='same',),\n            Conv2D(256, (3, 3), activation='relu', padding='same',),\n            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n            Conv2D(512, (3, 3), activation='relu', padding='same',),\n            Conv2D(512, (3, 3), activation='relu', padding='same',),\n            Conv2D(512, (3, 3), activation='relu', padding='same',),\n            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n            Conv2D(512, (3, 3), activation='relu', padding='same',),\n            Conv2D(512, (3, 3), activation='relu', padding='same',),\n            Conv2D(512, (3, 3), activation='relu', padding='same',),\n            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n            Flatten(),\n            Dense(4096, activation='relu'),\n            Dense(4096, activation='relu'),\n            Dense(1000, activation='softmax')\n        ])\n        return model\n    \n            \n    #Total params: 33,560,364    \n    def create_model_small(self):\n        \"\"\"\n        Build CNN model using img_width, img_height from fields.\n        \"\"\"\n        model=Sequential()\n        model.add(Conv2D(16, kernel_size=3, input_shape=(self.img_width, self.img_height,3), activation='relu', padding='same'))\n        model.add(MaxPooling2D(2))\n        model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n        model.add(Dropout(0.1))\n        model.add(Flatten())\n        model.add(Dense(64, activation = \"relu\"))        \n        # 1 y label\n        model.add(Dense(NUM_CLASSES, activation='softmax'))\n        #model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n        # We use a very small learning rate \n        model.compile(loss = 'categorical_crossentropy',\n              optimizer = RMSprop(lr = 0.001),\n              metrics = ['accuracy'])\n        return model\n    \n    def train(self, model, toy):\n        \"\"\"\n        Train the model\n        \"\"\"\n        if toy:\n            epochs=10\n            steps_per_epoch=10\n            #steps_per_epoch = self.nb_train_samples / self.batch_size\n            validation_steps=1\n            #validation_steps= self.nb_validation_samples / self.batch_size\n        else:\n            epochs=50\n            #steps_per_epoch=100\n            #steps_per_epoch=30 \n            \n            #steps_per_epoch=20\n            steps_per_epoch = self.nb_train_samples / self.batch_size\n            \n            #steps_per_epoch=10\n            #validation_steps=5\n            \n            #validation_steps=100\n            validation_steps= self.nb_validation_samples / self.batch_size\n\n\n        # val_accuracy should be try and compare with val_loss\n        # We'll stop training if no improvement after some epochs\n        earlystopper = EarlyStopping(monitor='val_loss', \n                            min_delta = 0, \n                            patience = 4,\n                            verbose = 1,\n                            restore_best_weights = True)\n\n        #reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n        #                           verbose=1, mode='max', min_lr=0.00001)\n        reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', #'loss',\n                              factor = 0.5,\n                              patience = 2,\n                              verbose = 1,\n                              min_lr=0.000001,\n                              min_delta = 0.0001)\n        # Save the best model during the traning\n        # loss,accuracy,val_loss,val_accuracy,lr\n        check_name = OUTPUT_DIR +  'best_'+self.model_name+'.h5'\n        print(check_name)\n        checkpointer = ModelCheckpoint( check_name\n                                        #,monitor='val_loss'\n                                        #,mode=\"min\"\n                                        ,monitor='val_accuracy'\n                                        ,mode=\"max\"                                       \n                                        ,verbose=1\n                                        ,save_best_only=True)\n                                        #,save_weights_only=True)\n\n        #callbacks = [ checkpointer,lr_callback]        \n            \n        callbacks = [earlystopper, checkpointer, lr_callback] #reduce_lr]\n        # Train\n        training = model.fit_generator(self.generators.train_generator\n                                ,epochs=epochs\n                                ,steps_per_epoch=steps_per_epoch\n                                ,validation_data=self.generators.val_generator\n                                ,validation_steps=validation_steps\n                                ,callbacks=callbacks\n                                ,use_multiprocessing=False\n                                ,shuffle=True\n                                ,verbose=True)     \n        \n        # Get the best saved weights\n        model.load_weights(check_name)\n        return training","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = trainer.create_model_3()  # Best ! 0.9219 Total params: 27,935,872  \n#model = trainer.create_model_2()   # good ! 0.8750 Total params: 7,049,804  \n#model = trainer.create_model_small() # worse 0.5625 Total params: 33,560,364\n#model = trainer.create_model()  # bad delete; Total params: 33,682,380\n#model = trainer.create_model(\"middle\") # bad Total params: 3,017,928\n\n#model = trainer.create_model(\"VGG16\") # 0.7344  Total params: 14,720,844\n#model = trainer.create_model(\"ResNet50V2\") # 0.8906 Total params: 23,589,388\n#model = trainer.create_model(\"InceptionResNetV2\") # 0.8906 Total params: Total params: 54,355,180\n\n#model = trainer.create_model(\"XceptionDenseNet121\") # 0.9062 Total params: 27,935,872\n\n# Create and train the model\ntrainer = ModelTrainer(generators)\nmodel = trainer.create_model(\"EfficientNetB7\") \nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\ntraining=trainer.train(model, toy)\nprint(\"Trained Time : \", time.time() - start)\n\n# training is a time consuming job, pls use this file for training only, use notebook-submit-only link as below for submission \n# the prevois output files( checkpoint weights) will be deleted when training, pls download and save at local, or upload to our private dataset \"pretrained-model\"\n\n# the best checkpoint should be \n# 1. download first, \n# 2. upload to pretrained-model dataset\n# 3. load in file below\n# https://www.kaggle.com/rebootshen/notebook-submit-only/edit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Evaluate trained model\nAlso put all evaluation code into one class for better code modularity.","metadata":{}},{"cell_type":"code","source":"# Saving model history ï¼š causing trouble for submission.csv!!\n#pd.DataFrame(training.history).to_csv('ModelHistory.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    \"\"\"\n    Evaluaion :predict on test data (not submission data from test folder)\n    and print reports, plot results etc.\n    \"\"\"\n     \n    def __init__(self, model, training, generator, y_true):\n        self.training = training\n        self.generator = generator\n        # predict the data\n        steps=5\n        self.y_pred_raw = model.predict(self.generator, steps=steps)\n        \n        self.y_pred = np.argmax(self.y_pred_raw, axis=1)\n        self.y_true=y_true[:len(self.y_pred)]\n        #print(self.y_pred_raw[0])\n        #print(self.y_pred)\n        #print(len(self.y_pred))\n        #print(self.y_true[0], y_true[0])\n        #print(y_true.shape, self.y_true.shape)\n        #print(y_true)\n        #print(y_true[:len(self.y_pred)] )\n    \n    def plot_history(self): \n        fig, ax1 = plt.subplots(figsize=(15, 8))\n        history = self.training\n\n        ax1.plot(history.history['loss'], 'r', label=\"training loss ({:.6f})\".format(history.history['loss'][-1]))\n        ax1.plot(history.history['val_loss'], 'r--', label=\"validation loss ({:.6f})\".format(history.history['val_loss'][-1]))\n        ax1.grid(True)\n        ax1.set_xlabel('iteration')\n        ax1.legend(loc=\"best\", fontsize=9)    \n        ax1.set_ylabel('loss', color='r')\n        ax1.tick_params('y', colors='r')\n\n        accname = \"accuracy\"\n        if accname in history.history:\n            ax2 = ax1.twinx()\n\n            ax2.plot(history.history[accname], 'b', label=\"training acc ({:.4f})\".format(history.history[accname][-1]))\n            ax2.plot(history.history['val_' + accname], 'b--', label=\"validation acc ({:.4f})\".format(history.history['val_' + accname][-1]))\n\n            ax2.legend(loc=\"best\", fontsize=12)\n            ax2.set_ylabel('acc', color='b')        \n            ax2.tick_params('y', colors='b')\n        \n    \"\"\"\n    Accuracy, evaluation\n    \"\"\"\n    def plot_history1(self):\n        \"\"\"\n        Plot training history\n        \"\"\"\n        ## Trained model analysis and evaluation\n        f, ax = plt.subplots(1,2, figsize=(15,5))\n        ax[0].plot(self.training.history['loss'], label=\"Loss\")\n        ax[0].plot(self.training.history['val_loss'], label=\"Validation loss\")\n        ax[0].set_title('Loss')\n        ax[0].set_xlabel('Epoch')\n        ax[0].set_ylabel('Loss')\n        ax[0].legend()\n\n        # Accuracy\n        ax[1].plot(self.training.history['accuracy'], label=\"Accuracy\")\n        ax[1].plot(self.training.history['val_accuracy'], label=\"Validation accuracy\")\n        ax[1].set_title('Accuracy')\n        ax[1].set_xlabel('Epoch')\n        ax[1].set_ylabel('Accuracy')\n        ax[1].legend()\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_roc(self):\n        #y_pred_keras = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\n        # Calculate roc\n        fpr_keras, tpr_keras, thresholds_keras = roc_curve(self.y_true, self.y_pred)\n        auc_keras = auc(fpr_keras, tpr_keras)\n        \n        plt.figure(1)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n        plt.xlabel('False positive rate')\n        plt.ylabel('True positive rate')\n        plt.title('ROC curve')\n        plt.legend(loc='best')\n        plt.show()\n        \n    def print_report(self):\n        \"\"\"\n        Predict and evaluate using ground truth from labels\n        Test generator did not shuffle \n        and we can use true labels for comparison\n        \"\"\"\n        #Print classification report\n        #print(self.y_true.shape, self.y_pred.shape)\n        #print(self.y_true[0], self.y_pred[0])\n        #print(self.generator.classes)\n        #print(metrics.classification_report(self.y_true, self.y_pred))\n        \n        class_labels = self.generator.class_indices\n        print(class_labels)\n        class_labels = {v: k for k, v in class_labels.items()}\n        classes = list(class_labels.values())\n        #print(classes)        \n        #print(len(self.generator.classes))\n        \n        y_pred = []\n        for item in self.y_pred:\n            y_pred.append(classes[item])\n            \n        \n        self.y_pred = y_pred \n        print(metrics.classification_report(self.y_true, self.y_pred))\n        \n\n        \n        #print('Confusion Matrix')\n        #print(confusion_matrix(validation_generator.classes, y_pred))\n        #print('Classification Report')\n        #target_names = list(class_labels.values())\n        #print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n\n# Create evaluator instance\n#evaluator = Evaluator(model, training, generators.test_generator, test_df.labels.values)\n\n# Draw accuracy and loss charts\n#evaluator.plot_history()\n\n# ROC curve\n#evaluator.plot_roc()\n\n# Classification report\n#evaluator.print_report()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Submission\n>Use ImageDataGenerator to reduce memory usage. My initial idea was to use generator.flow_from_directory for input/test folder but it didn't work for me. Quick fix is to use generator.flow_from_dataframe on dataframe with list of filenames.","metadata":{}},{"cell_type":"code","source":"class Submitter:\n    \"\"\"\n    Predict and submit\n    \"\"\"\n    def __init__(self, model, img_size):\n        self.model = model\n        batch_size=BATCH_SIZE\n        print(\"Initializing submitter\")\n        #Submission generator\n        # flow_from_directory for input/test didn't work for me, so quick fix is to use flow_from_dataframe with list of files\n        # Load list of files from test folder into dataframe\n        self.test_files_df=pd.DataFrame()\n        TEST_DIR = INPUT_DIR + 'test_images/'\n        self.test_files_df['image']=os.listdir(TEST_DIR)\n        print(\"Loaded test files list\")\n        \n        \n        # Create generator in it\n        #self.generator=ImageDataGenerator(rescale=1./255.).flow_from_dataframe(\n        _test_datagen=ImageDataGenerator(rescale=1./255.)\n        self.generator = _test_datagen.flow_from_dataframe(\n                    dataframe=self.test_files_df,\n                    directory=TEST_DIR,\n                    x_col=\"image\",\n                    y_col=None,\n                    #has_ext=True,\n                    class_mode=None,\n                    batch_size=batch_size,\n                    seed=42,\n                    shuffle=False,\n                    target_size=img_size)    \n        \n        print('Submission generator created')    \n\n\n    def predict_for_submit(self):\n        \"\"\"\n        Predict submission test data and form dataframe to submit\n        \"\"\"\n        print(\"Forming submission dataframe...\")\n        # Predict\n        y_pred = self.model.predict(self.generator)\n        y_pred = np.argmax(y_pred, axis=1)\n        print(y_pred)\n        \n        \n        y_pred_str = []\n        for item in y_pred:\n            y_pred_str.append(classes[item])\n        \n        self.test_files_df['labels'] = y_pred_str\n        # Write to csv\n        self.test_files_df.to_csv('./submission.csv', index=False)\n        print(\"Submission completed: written submission.csv\")\n        return self.test_files_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not toy:\n    # Get dataframe for submission\n    submitter = Submitter(model, IMAGE_SIZE)\n    submission_df = submitter.predict_for_submit()     \n    submission_df.head()\nelse:\n    submission_df = pd.DataFrame()\n    print(\"Do not submit in toy mode\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sizes = pd.DataFrame({'Data type':['Train', 'Test', 'Submission'], \n                      'Row count':[len(train_df.values), len(test_df.values), len(submission_df.values)]})\n\nsns.barplot(x='Data type', y='Row count', data=df_sizes)\nplt.title('Rows in data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
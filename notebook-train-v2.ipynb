{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    ">Just one more kernel with CNN in this competition :)\n",
    "To work with large data, use ImageDataGenerator.flow_from_dataframe as input for model.\n",
    "First, import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Libraries\n",
    "#####################################\n",
    "# Common libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Image processing\n",
    "import imageio\n",
    "import PIL\n",
    "import cv2\n",
    "import skimage.transform\n",
    "#from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "# Charts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML, statistics\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Tensorflow\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Settings\n",
    "#############################################\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "#plt.style.use('seaborn')\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "INPUT_DIR = \"../input/plant-pathology-2021-fgvc8/\"\n",
    "PRETRAINED_DIR = \"../input/pretrained-model/\"\n",
    "OUTPUT_DIR =\"./\"\n",
    "\n",
    "#OUTPUT_DIR = \"./model/\"\n",
    "\n",
    "TRAIN_DATA_DIR = INPUT_DIR + 'train_images'\n",
    "#TRAIN_DATA_DIR = INPUT_DIR + 'img_sz_512'\n",
    "\n",
    "IMSIZES = (224, 240, 260, 300, 380, 456, 528, 600)\n",
    "\n",
    "#NUM_CLASSES = 7 #12\n",
    "\n",
    "NUM_CLASSES = 6 #12\n",
    "IMAGE_SIZE= (224,224)  #(256,256)\n",
    "\n",
    "BATCH_SIZE = 32 #32\n",
    "TRAIN_BATCH_SIZE = 1024 #512\n",
    "\n",
    "print(os.listdir(OUTPUT_DIR))\n",
    "print(os.listdir(INPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=False\n",
    "\n",
    "all_df = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "if toy:\n",
    "    all_df = all_df.sample(5000)\n",
    "\n",
    "\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRAIN = all_df.shape[0]  #15930 #4275 #32706\n",
    "NB_VALID = NB_TRAIN * 0.1 #1770 #475 #10902"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    ">Check labels balancing\n",
    "Check whether labels are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any null values in the dataset\n",
    "all_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the column data type\n",
    "all_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.labels.value_counts().plot(kind='bar')\n",
    "plt.title('Labels counts')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print(all_df.labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'] = all_df['image']\n",
    "train['labels'] = all_df['labels'].apply(lambda string: string.split(' '))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = list(train['labels'])\n",
    "mlb = MultiLabelBinarizer()\n",
    "trainx = pd.DataFrame(mlb.fit_transform(s), columns=mlb.classes_, index=train.index)\n",
    "print(trainx.columns)\n",
    "print(trainx.sum())\n",
    "\n",
    "labels = list(trainx.sum().keys())\n",
    "print(labels)\n",
    "label_counts = trainx.sum().values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,6))\n",
    "\n",
    "sns.barplot(x= labels, y= label_counts, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "scab                               1251\n",
    "healthy                            1209\n",
    "frog_eye_leaf_spot                  881\n",
    "rust                                515\n",
    "complex                             449\n",
    "powdery_mildew                      320\n",
    "scab frog_eye_leaf_spot             190\n",
    "scab frog_eye_leaf_spot complex      53\n",
    "rust frog_eye_leaf_spot              41\n",
    "frog_eye_leaf_spot complex           40\n",
    "rust complex                         29\n",
    "powdery_mildew complex               22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are unbalanced in train dataset, will figure it out in preprocessing stage.\n",
    "\n",
    "## View sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(26,20))\n",
    "\n",
    "for i in range(1, 13):\n",
    "    \n",
    "    rand =  random.randrange(1, 18000)\n",
    "    sample = os.path.join('../input/plant-pathology-2021-fgvc8/train_images/', train['image'][rand])\n",
    "    \n",
    "    img = PIL.Image.open(sample)\n",
    "    \n",
    "    ax = fig1.add_subplot(4,3,i)\n",
    "    ax.imshow(img)\n",
    "    #print(img.size)\n",
    "    \n",
    "    title = f\"{train['labels'][rand]}{img.size}\"\n",
    "    plt.title(title)\n",
    "    \n",
    "    fig1.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preparation\n",
    ">Here we are going to balance dataset and prepare image generator\n",
    "\n",
    "## Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    \"\"\"\n",
    "    Train/test\n",
    "    \"\"\"\n",
    "    def train_test_split(self, all_df):\n",
    "        \"\"\"\n",
    "        Balanced split to train, test and val\n",
    "        \"\"\"\n",
    "        # Split to train and test before balancing\n",
    "        train_df, test_df = train_test_split(all_df, random_state=24, test_size=0.05, shuffle=False)\n",
    "        print(train_df.shape)\n",
    "#         # Split train to train and validation datasets\n",
    "#         train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=24)\n",
    "        # Number of samples in each category\n",
    "        \n",
    "        #ncat_bal = train_df['labels'].value_counts().max()\n",
    "#        #ncat_bal = int(len(train_df)/train_df['label'].cat.categories.size)\n",
    "        #train_df = train_df.groupby('labels', as_index=False).apply(lambda g:  g.sample(ncat_bal, replace=True, random_state=24)).reset_index(drop=True)\n",
    "        #print(train_df.shape, ncat_bal)\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def plot_balanced(self, train_df, all_df):\n",
    "        \"\"\"\n",
    "        Plot samples per category before and after balancing\n",
    "        \"\"\"\n",
    "        f, axs = plt.subplots(1,2,figsize=(15,4))\n",
    "        # Before balancing\n",
    "        all_df.labels.value_counts().plot(kind='bar', ax=axs[0])\n",
    "        axs[0].set_title('All labels')\n",
    "        axs[0].set_xlabel('Label')\n",
    "        axs[0].set_ylabel('Count')\n",
    "        # After balancing\n",
    "        train_df.labels.value_counts().plot(kind='bar', ax=axs[1])\n",
    "        axs[1].set_title('Train labels after balancing')\n",
    "        axs[1].set_xlabel('Label')\n",
    "        axs[1].set_ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Train/test/validation split with balanced labels in train\n",
    "data_prep = DataPreparation()\n",
    "#train_df, test_df = data_prep.train_test_split(all_df)\n",
    "train_df, test_df = data_prep.train_test_split(train)\n",
    "\n",
    "# Plot before and after balancing\n",
    "data_prep.plot_balanced(train_df, all_df)\n",
    "print(all_df.shape,train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image generator\n",
    ">Keras ImageDataGenerator can work with dataframe of file names. Our train, validation and test dataframes contain file name in id column and ImageDataGenerator can understand id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generators:\n",
    "    \"\"\"\n",
    "    Train, validation and test generators\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.batch_size= BATCH_SIZE #16 #1024 #32 #8\n",
    "        self.img_size= IMAGE_SIZE #(256,256)#(512,512)#(192,128) #(96,64) (384,256)  #(4000,2672)\n",
    "\n",
    "        # Base train/validation generator\n",
    "        _datagen = ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            validation_split=0.1,\n",
    "            brightness_range=(0.8, 1.2),\n",
    "            #featurewise_center=False,\n",
    "            #featurewise_std_normalization=False,\n",
    "            fill_mode='nearest',\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10, #180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.15, # Randomly zoom image \n",
    "            width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True  # randomly flip images\n",
    "            )\n",
    " \n",
    "        # Train generator\n",
    "        self.train_generator = _datagen.flow_from_dataframe(\n",
    "            dataframe=train_df, #all_df, #\n",
    "            directory=TRAIN_DATA_DIR,\n",
    "            x_col=\"image\",\n",
    "            y_col=\"labels\",\n",
    "            #has_ext=False,\n",
    "            \n",
    "            subset=\"training\",\n",
    "            batch_size=self.batch_size,\n",
    "            seed=42,\n",
    "            shuffle=False, #True,\n",
    "            class_mode=\"categorical\",\n",
    "            target_size=self.img_size)\n",
    "        print('Train generator created')\n",
    "        # Validation generator\n",
    "        self.val_generator = _datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,#test_df ,#\n",
    "            directory=TRAIN_DATA_DIR,\n",
    "            x_col=\"image\",\n",
    "            y_col=\"labels\",\n",
    "            #has_ext=False,\n",
    "            \n",
    "            subset=\"validation\",\n",
    "            batch_size=self.batch_size,\n",
    "            seed=42,\n",
    "            shuffle=False, #True,\n",
    "            class_mode=\"categorical\",\n",
    "            target_size=self.img_size)    \n",
    "        print('Validation generator created')\n",
    "        \n",
    "        \n",
    "        # Test generator\n",
    "        _test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "        self.test_generator = _test_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=TRAIN_DATA_DIR,\n",
    "            x_col=\"image\",\n",
    "            y_col='labels',\n",
    "            #has_ext=False,\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=self.batch_size,\n",
    "            seed=42,\n",
    "            shuffle=False,\n",
    "            target_size=self.img_size)     \n",
    "        print('Test generator created')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# Create generators   \n",
    "#(43608, 2) (4658, 2)\n",
    "generators = Generators(train_df, test_df)\n",
    "print(\"Generators created\")\n",
    "\n",
    "#(18632, 2) (17700, 2) (932, 2)\n",
    "#15930 , 1770, 932\n",
    "\n",
    "'''\n",
    "#https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538\n",
    "#K-Fold Cross Validation for Deep Learning Models \n",
    "Y = train_df[['labels']]\n",
    "kf = KFold(n_splits = 5) \n",
    "skf = StratifiedKFold(n_split = 5, random_state = 7, shuffle = True) \n",
    "\n",
    "n = train_df.shape()[0]\n",
    "for train_index, val_index in kf.split(np.zeros(n),Y):\n",
    "    training_data = train_df.iloc[train_index]\n",
    "    validation_data = train_df.iloc[val_index]\n",
    "    \n",
    "    generators = Generators(training_data, validation_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generators.train_generator.next())\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generators.train_generator.classes)\n",
    "print(generators.train_generator.class_indices)\n",
    "class_labels = generators.train_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "print(class_labels)\n",
    "classes = list(class_labels.values())\n",
    "print(classes)        \n",
    "#print(len(self.generator.classes))\n",
    "\n",
    "#y_pred = []\n",
    "#for item in self.y_pred:\n",
    "#    y_pred.append(label_batch)\n",
    "def get_class(one_hot):\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if one_hot[i] == 1:\n",
    "            return classes[i]\n",
    "    \n",
    "            \n",
    "            \n",
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(20, 14))\n",
    "columns = 2\n",
    "rows = 4\n",
    "plt.title('Image Class')\n",
    "plt.axis('off')\n",
    "for i in range(1, columns*rows):\n",
    "    \n",
    "    img_batch, label_batch = generators.train_generator.next()\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    \n",
    "    plt.title(get_class(label_batch[i]))  \n",
    "    plt.imshow(img_batch[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create and train the model\n",
    ">Put all creation and training code in one class. Will experiment with model architecture later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_START = 0.00001\n",
    "LR_MAX = 0.0001 \n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 15\n",
    "LR_SUSTAIN_EPOCHS = 3\n",
    "LR_EXP_DECAY = .8\n",
    "EPOCHS = 100\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Create and fit the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generators):\n",
    "        #(18632, 2) (17700, 2) (932, 2)\n",
    "        #15930 , 1770, 932\n",
    "        \n",
    "        self.nb_train_samples = NB_TRAIN  #15930 #4275 #32706\n",
    "        self.nb_validation_samples = NB_VALID #1770 #475 #10902\n",
    "        self.batch_size =  TRAIN_BATCH_SIZE #512 #512 #32 #1024   #32\n",
    "               \n",
    "        \n",
    "        self.generators = generators\n",
    "        self.img_width = IMAGE_SIZE[0]\n",
    "        self.img_height = IMAGE_SIZE[1]\n",
    "        print(self.img_width,self.img_height)\n",
    "        \n",
    "    \n",
    "    def create_model(self, model_name):\n",
    "        #will preload and consume lots memory\n",
    "        #dict_model = {\n",
    "        #    \"Xception\": self.create_model_1(),\n",
    "        #    \"DenseNet121\": self.create_model_2(),\n",
    "        #    \"XceptionDenseNet121\": self.create_model_3(),\n",
    "        #    \"InceptionResNetV2\": self.create_model_4(),\n",
    "        #    \"ResNet50V2\": self.create_model_5(),\n",
    "        #    \"VGG16\": self.create_model_6(),\n",
    "        #    \"middle\": self.create_model_middle(),\n",
    "        #    \"small\": self.create_model_small(),\n",
    "        #}    \n",
    "        #print(\"Creating \"+model_name)\n",
    "        if model_name == \"Xception\" :\n",
    "                model = self.create_model_1()\n",
    "        elif model_name == \"EfficientNetB7\": \n",
    "                model = self.create_model_0()                \n",
    "        elif model_name == \"DenseNet121\": \n",
    "                model = self.create_model_2()\n",
    "        elif model_name == \"XceptionDenseNet121\": \n",
    "                model = self.create_model_3()\n",
    "        elif model_name == \"InceptionResNetV2\": \n",
    "                model = self.create_model_4()\n",
    "        elif model_name == \"ResNet50V2\": \n",
    "                model = self.create_model_5()\n",
    "        elif model_name == \"VGG16\": \n",
    "                model = self.create_model_6()\n",
    "        elif model_name == \"middle\": \n",
    "                model = self.create_model_middle()\n",
    "        elif model_name == \"small\": \n",
    "                model = self.create_model_small()\n",
    "        else:\n",
    "                print(\"Model not found\")\n",
    "        self.model_name = model_name\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def create_model_0(self):\n",
    "        path0 = PRETRAINED_DIR + 'efficientnetb7_notop.h5'\n",
    "        pretrained_model = tf.keras.applications.EfficientNetB7(include_top=False, weights=path0, #drop_connect_rate=0.4,\n",
    "                                                                 input_shape=(self.img_width, self.img_height, 3))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            pretrained_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Freezing the weights\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable=False\n",
    "    \n",
    "        return model    \n",
    "    \n",
    "    #Total params: 20,886,068\n",
    "    def create_model_1(self):\n",
    "        path1 = PRETRAINED_DIR + 'xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        pretrained_model = tf.keras.applications.xception.Xception(include_top=False, weights=path1, input_shape=(self.img_width, self.img_height, 3))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            pretrained_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def create_model_2(self):\n",
    "        path2 = PRETRAINED_DIR + 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        pretrained_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights=path2,input_shape=(self.img_width, self.img_height, 3))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            pretrained_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "        ])\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def create_model_3(self):\n",
    "        path1 = PRETRAINED_DIR + 'xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        xception_model = tf.keras.models.Sequential([\n",
    "           tf.keras.applications.xception.Xception(include_top=False, weights=path1, input_shape=(self.img_width, self.img_height, 3)),#(512, 512, 3)),\n",
    "           tf.keras.layers.GlobalAveragePooling2D(),\n",
    "           #tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "           tf.keras.layers.Dense(NUM_CLASSES,activation='sigmoid')\n",
    "        ])\n",
    "        # Freezing the weights\n",
    "        #for layer in xception_model.layers[:-1]:\n",
    "        #    layer.trainable=False\n",
    "            \n",
    "        #xception_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        #xception_model.summary()\n",
    "        path2 = PRETRAINED_DIR + 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        densenet_model = tf.keras.models.Sequential([\n",
    "            tf.keras.applications.densenet.DenseNet121(include_top=False, weights=path2,input_shape=(self.img_width, self.img_height, 3)),#(512, 512, 3)),\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            #tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='sigmoid')\n",
    "        ])\n",
    "        #for layer in densenet_model.layers[:-1]:\n",
    "        #    layer.trainable=False\n",
    "        #densenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        #densenet_model.summary()\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(self.img_width, self.img_height, 3)) #(512, 512, 3))\n",
    "\n",
    "        xception_output = xception_model(inputs)\n",
    "        densenet_output = densenet_model(inputs)\n",
    "\n",
    "        outputs = tf.keras.layers.average([densenet_output, xception_output])\n",
    "\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    " \n",
    "    def create_model_4(self):\n",
    "        pretrained_model = tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(self.img_width, self.img_height, 3))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            pretrained_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def create_model_5(self):\n",
    "        pretrained_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_shape=(self.img_width, self.img_height, 3))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            pretrained_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "        ])\n",
    "    \n",
    "        return model\n",
    "\n",
    "    #Total params: 14,720,844\n",
    "    def create_model_6(self):\n",
    "        pretrained_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(self.img_width, self.img_height, 3))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            pretrained_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')\n",
    "        ])\n",
    "        return model    \n",
    "    \n",
    "    def create_model_middle(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Padding = 'same'  results in padding the input such that\n",
    "        # the output has the same length as the original input\n",
    "        model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                         input_shape= (self.img_width, self.img_height, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(12))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(NUM_CLASSES))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def create_model_vgg16(self):\n",
    "        input_shape = (224, 224, 3)\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv2D(64, (3, 3), input_shape=input_shape, padding='same',\n",
    "                   activation='relu'),\n",
    "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
    "            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "            Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "            Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dense(1000, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "            \n",
    "    #Total params: 33,560,364    \n",
    "    def create_model_small(self):\n",
    "        \"\"\"\n",
    "        Build CNN model using img_width, img_height from fields.\n",
    "        \"\"\"\n",
    "        model=Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=3, input_shape=(self.img_width, self.img_height,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(2))\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation = \"relu\"))        \n",
    "        # 1 y label\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        #model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # We use a very small learning rate \n",
    "        model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train(self, model, toy):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        if toy:\n",
    "            epochs=10\n",
    "            steps_per_epoch=10\n",
    "            #steps_per_epoch = self.nb_train_samples / self.batch_size\n",
    "            validation_steps=1\n",
    "            #validation_steps= self.nb_validation_samples / self.batch_size\n",
    "        else:\n",
    "            epochs=50\n",
    "            #steps_per_epoch=100\n",
    "            #steps_per_epoch=30 \n",
    "            \n",
    "            #steps_per_epoch=20\n",
    "            steps_per_epoch = self.nb_train_samples / self.batch_size\n",
    "            \n",
    "            #steps_per_epoch=10\n",
    "            #validation_steps=5\n",
    "            \n",
    "            #validation_steps=100\n",
    "            validation_steps= self.nb_validation_samples / self.batch_size\n",
    "\n",
    "\n",
    "        # val_accuracy should be try and compare with val_loss\n",
    "        # We'll stop training if no improvement after some epochs\n",
    "        earlystopper = EarlyStopping(monitor='val_loss', \n",
    "                            min_delta = 0, \n",
    "                            patience = 10,\n",
    "                            verbose = 1,\n",
    "                            restore_best_weights = True)\n",
    "\n",
    "        #reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n",
    "        #                           verbose=1, mode='max', min_lr=0.00001)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', #'loss',\n",
    "                              factor = 0.5,\n",
    "                              patience = 2,\n",
    "                              verbose = 1,\n",
    "                              min_lr=0.000001,\n",
    "                              min_delta = 0.0001)\n",
    "        # Save the best model during the traning\n",
    "        # loss,accuracy,val_loss,val_accuracy,lr\n",
    "        check_name = OUTPUT_DIR +  'best_'+self.model_name+'.h5'\n",
    "        print(check_name)\n",
    "        checkpointer = ModelCheckpoint( check_name\n",
    "                                        #,monitor='val_loss'\n",
    "                                        #,mode=\"min\"\n",
    "                                        ,monitor='val_accuracy'\n",
    "                                        ,mode=\"max\"                                       \n",
    "                                        ,verbose=1\n",
    "                                        ,save_best_only=True)\n",
    "                                        #,save_weights_only=True)\n",
    "\n",
    "        #callbacks = [ checkpointer,lr_callback]        \n",
    "            \n",
    "        callbacks = [earlystopper, checkpointer, reduce_lr] #lr_callback] #\n",
    "        # Train\n",
    "        training = model.fit(self.generators.train_generator\n",
    "                                ,epochs=epochs\n",
    "                                ,steps_per_epoch=steps_per_epoch\n",
    "                                ,validation_data=self.generators.val_generator\n",
    "                                ,validation_steps=validation_steps\n",
    "                                ,callbacks=callbacks\n",
    "                                ,use_multiprocessing=False\n",
    "                                ,shuffle=True\n",
    "                                ,verbose=True)     \n",
    "        \n",
    "        # Get the best saved weights\n",
    "        #model.load_weights(check_name)\n",
    "        return training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = trainer.create_model_3()  # Best ! 0.9219 Total params: 27,935,872  \n",
    "#model = trainer.create_model_2()   # good ! 0.8750 Total params: 7,049,804  \n",
    "#model = trainer.create_model_small() # worse 0.5625 Total params: 33,560,364\n",
    "#model = trainer.create_model()  # bad delete; Total params: 33,682,380\n",
    "#model = trainer.create_model(\"middle\") # bad Total params: 3,017,928\n",
    "\n",
    "#model = trainer.create_model(\"VGG16\") # 0.7344  Total params: 14,720,844\n",
    "#model = trainer.create_model(\"ResNet50V2\") # 0.8906 Total params: 23,589,388\n",
    "#model = trainer.create_model(\"InceptionResNetV2\") # 0.8906 Total params: Total params: 54,355,180\n",
    "\n",
    "#model = trainer.create_model(\"XceptionDenseNet121\") # 0.9062 Total params: 27,935,872\n",
    "\n",
    "# Create and train the model\n",
    "trainer = ModelTrainer(generators)\n",
    "#model = trainer.create_model(\"EfficientNetB7\")   # Total params: 64,128,419\n",
    "model = trainer.create_model(\"XceptionDenseNet121\")\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#f1 = tfa.metrics.F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "#model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=1e-4),  metrics= [f1])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "training=trainer.train(model, toy)\n",
    "print(\"Trained Time : \", time.time() - start)\n",
    "        \n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "#tf.keras.models.save_model(model, './model/plant_model.hdf5')\n",
    "#model.load_weights('../working/best_XceptionDenseNet121.h5')\n",
    "\n",
    "\n",
    "#Epoch 1/100\n",
    "\n",
    "#Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
    "#31/31 [==============================] - 62s 2s/step - loss: 2.2864 - accuracy: 0.2022 - val_loss: 2.2638 - val_accuracy: 0.3125\n",
    "\n",
    "#model_2\n",
    "#Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001.\n",
    "#31/31 [==============================] - 363s 12s/step - loss: 0.5241 - accuracy: 0.8358 - val_loss: 0.5352 - val_accuracy: 0.8750\n",
    "\n",
    "#model_3\n",
    "#Epoch 00029: LearningRateScheduler reducing learning rate to 1.9663676416000005e-05.\n",
    "#31/31 [==============================] - 666s 21s/step - loss: 0.4607 - accuracy: 0.8552 - val_loss: 0.3860 - val_accuracy: 0.9219\n",
    "\n",
    "\n",
    "#model_small\n",
    "#Epoch 00036: LearningRateScheduler reducing learning rate to 1.2026619832316725e-05.\n",
    "#31/31 [==============================] - 50s 2s/step - loss: 1.6877 - accuracy: 0.3644 - val_loss: 1.3935 - val_accuracy: 0.5625\n",
    "\n",
    "#model = trainer.create_model_1() # Total params: 20,886,068\n",
    "#Epoch 00033: LearningRateScheduler reducing learning rate to 1.3958241859993605e-05.\n",
    "#31/31 [==============================] - 329s 11s/step - loss: 0.5072 - accuracy: 0.8469 - val_loss: 0.4209 - val_accuracy: 0.9062\n",
    "\n",
    "#model = trainer.create_model(\"VGG16\") # Total params: 14,720,844\n",
    "#31/31 [==============================] - 361s 12s/step - loss: 1.1590 - accuracy: 0.6339 - val_loss: 0.8088 - val_accuracy: 0.7344\n",
    "\n",
    "#model = trainer.create_model(\"ResNet50V2\") #Total params: 23,589,388\n",
    "#Epoch 00025: LearningRateScheduler reducing learning rate to 3.359296000000001e-05.\n",
    "#31/31 [==============================] - 257s 8s/step - loss: 0.7078 - accuracy: 0.7953 - val_loss: 0.4911 - val_accuracy: 0.8906\n",
    "\n",
    "#model = trainer.create_model(\"InceptionResNetV2\") # 0.8906 Total params: Total params: 54,355,180\n",
    "#Epoch 00025: LearningRateScheduler reducing learning rate to 3.359296000000001e-05.\n",
    "#31/31 [==============================] - 257s 8s/step - loss: 0.7078 - accuracy: 0.7953 - val_loss: 0.4911 - val_accuracy: 0.8906\n",
    "\n",
    "#model = trainer.create_model(\"XceptionDenseNet121\") # 0.9062 Total params: 27,935,872\n",
    "#Epoch 00025: LearningRateScheduler reducing learning rate to 3.359296000000001e-05.\n",
    "#31/31 [==============================] - 676s 22s/step - loss: 0.5947 - accuracy: 0.8298 - val_loss: 0.4676 - val_accuracy: 0.9062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate trained model\n",
    "Also put all evaluation code into one class for better code modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model history\n",
    "#pd.DataFrame(training.history).to_csv('ModelHistory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluaion :predict on test data (not submission data from test folder)\n",
    "    and print reports, plot results etc.\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, model, training, generator, y_true):\n",
    "        self.training = training\n",
    "        self.generator = generator\n",
    "        # predict the data\n",
    "        steps=5\n",
    "        preds = model.predict(self.generator, steps=steps)\n",
    "        print(preds)\n",
    "        preds = preds.tolist()\n",
    "\n",
    "        ####\n",
    "\n",
    "        indices = []\n",
    "        for pred in preds:\n",
    "            temp = []\n",
    "            for category in pred:\n",
    "                if category>=0.3:\n",
    "                    temp.append(pred.index(category))\n",
    "            if temp!=[]:\n",
    "                indices.append(temp)\n",
    "            else:\n",
    "                temp.append(np.argmax(pred))\n",
    "                indices.append(temp)\n",
    "\n",
    "        print(indices)\n",
    "        \n",
    "\n",
    "\n",
    "        testlabels = []\n",
    "\n",
    "\n",
    "        for image in indices:\n",
    "            temp = []\n",
    "            for i in image:\n",
    "                temp.append(str(class_labels[i]))\n",
    "            testlabels.append(' '.join(temp))\n",
    "\n",
    "        print(testlabels)\n",
    "\n",
    "                \n",
    "        \n",
    "        self.y_pred = testlabels\n",
    "        self.y_true = y_true[:len(self.y_pred)]\n",
    "        \n",
    "        #print(self.y_pred_raw[0])\n",
    "        #print(self.y_pred)\n",
    "        #print(len(self.y_pred))\n",
    "        #print(self.y_true[0], y_true[0])\n",
    "        #print(y_true.shape, self.y_true.shape)\n",
    "        #print(y_true)\n",
    "        #print(y_true[:len(self.y_pred)] )\n",
    "    \n",
    "    def plot_history(self): \n",
    "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "        history = self.training\n",
    "\n",
    "        ax1.plot(history.history['loss'], 'r', label=\"training loss ({:.6f})\".format(history.history['loss'][-1]))\n",
    "        ax1.plot(history.history['val_loss'], 'r--', label=\"validation loss ({:.6f})\".format(history.history['val_loss'][-1]))\n",
    "        ax1.grid(True)\n",
    "        ax1.set_xlabel('iteration')\n",
    "        ax1.legend(loc=\"best\", fontsize=9)    \n",
    "        ax1.set_ylabel('loss', color='r')\n",
    "        ax1.tick_params('y', colors='r')\n",
    "\n",
    "        accname = \"accuracy\"\n",
    "        if accname in history.history:\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            ax2.plot(history.history[accname], 'b', label=\"training acc ({:.4f})\".format(history.history[accname][-1]))\n",
    "            ax2.plot(history.history['val_' + accname], 'b--', label=\"validation acc ({:.4f})\".format(history.history['val_' + accname][-1]))\n",
    "\n",
    "            ax2.legend(loc=\"best\", fontsize=12)\n",
    "            ax2.set_ylabel('acc', color='b')        \n",
    "            ax2.tick_params('y', colors='b')\n",
    "        \n",
    "    \"\"\"\n",
    "    Accuracy, evaluation\n",
    "    \"\"\"\n",
    "    def plot_history1(self):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \"\"\"\n",
    "        ## Trained model analysis and evaluation\n",
    "        f, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "        ax[0].plot(self.training.history['loss'], label=\"Loss\")\n",
    "        ax[0].plot(self.training.history['val_loss'], label=\"Validation loss\")\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Accuracy\n",
    "        ax[1].plot(self.training.history['accuracy'], label=\"Accuracy\")\n",
    "        ax[1].plot(self.training.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "        ax[1].set_title('Accuracy')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_roc(self):\n",
    "        #y_pred_keras = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\n",
    "        # Calculate roc\n",
    "        fpr_keras, tpr_keras, thresholds_keras = roc_curve(self.y_true, self.y_pred)\n",
    "        auc_keras = auc(fpr_keras, tpr_keras)\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    def print_report(self):\n",
    "        \"\"\"\n",
    "        Predict and evaluate using ground truth from labels\n",
    "        Test generator did not shuffle \n",
    "        and we can use true labels for comparison\n",
    "        \"\"\"\n",
    "        #Print classification report\n",
    "        #print(self.y_true.shape, self.y_pred.shape)\n",
    "        #print(self.y_true[0], self.y_pred[0])\n",
    "        #print(self.generator.classes)\n",
    "        #print(metrics.classification_report(self.y_true, self.y_pred))\n",
    "        \n",
    "        #class_labels = self.generator.class_indices\n",
    "        #print(class_labels)\n",
    "        #class_labels = {v: k for k, v in class_labels.items()}\n",
    "        #classes = list(class_labels.values())\n",
    "        #y_pred = []\n",
    "        #for item in self.y_pred:\n",
    "        #    y_pred.append(classes[item])\n",
    "            \n",
    "        \n",
    "        #self.y_pred = y_pred \n",
    "        print(metrics.classification_report(self.y_true, self.y_pred))\n",
    "        \n",
    "\n",
    "        \n",
    "        #print('Confusion Matrix')\n",
    "        #print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "        #print('Classification Report')\n",
    "        #target_names = list(class_labels.values())\n",
    "        #print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = Evaluator(model, training, generators.test_generator, test_df.labels.values)\n",
    "\n",
    "# Draw accuracy and loss charts\n",
    "evaluator.plot_history()\n",
    "\n",
    "# ROC curve\n",
    "#evaluator.plot_roc()\n",
    "\n",
    "# Classification report\n",
    "#evaluator.print_report()\n",
    "\n",
    "#print(self.y_pred_raw[0])  # 12 proba\n",
    "#print(self.y_pred) # 160 pred result\n",
    "#print(len(self.y_pred)) #160\n",
    "#print(self.y_true[0], y_true[0]) #frog_eye_leaf_spot frog_eye_leaf_spot\n",
    "#print(y_true.shape) #250\n",
    "#print(y_true) # 250 labels string\n",
    "#print(y_true[:len(self.y_pred)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator.y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission\n",
    ">Use ImageDataGenerator to reduce memory usage. My initial idea was to use generator.flow_from_directory for input/test folder but it didn't work for me. Quick fix is to use generator.flow_from_dataframe on dataframe with list of filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Submitter:\n",
    "    \"\"\"\n",
    "    Predict and submit\n",
    "    \"\"\"\n",
    "    def __init__(self, model, img_size):\n",
    "        self.model = model\n",
    "        batch_size=BATCH_SIZE\n",
    "        print(\"Initializing submitter\")\n",
    "        #Submission generator\n",
    "        # flow_from_directory for input/test didn't work for me, so quick fix is to use flow_from_dataframe with list of files\n",
    "        # Load list of files from test folder into dataframe\n",
    "        self.test_files_df=pd.DataFrame()\n",
    "        TEST_DIR = INPUT_DIR + 'test_images/'\n",
    "        self.test_files_df['image']=os.listdir(TEST_DIR)\n",
    "        print(\"Loaded test files list\")\n",
    "        \n",
    "        \n",
    "        # Create generator in it\n",
    "        #self.generator=ImageDataGenerator(rescale=1./255.).flow_from_dataframe(\n",
    "        _test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "        self.generator = _test_datagen.flow_from_dataframe(\n",
    "                    dataframe=self.test_files_df,\n",
    "                    directory=TEST_DIR,\n",
    "                    x_col=\"image\",\n",
    "                    y_col=None,\n",
    "                    #has_ext=True,\n",
    "                    class_mode=None,\n",
    "                    batch_size=batch_size,\n",
    "                    seed=42,\n",
    "                    shuffle=False,\n",
    "                    target_size=img_size)    \n",
    "        \n",
    "        print('Submission generator created')    \n",
    "\n",
    "\n",
    "    def predict_for_submit(self):\n",
    "        \"\"\"\n",
    "        Predict submission test data and form dataframe to submit\n",
    "        \"\"\"\n",
    "        print(\"Forming submission dataframe...\")\n",
    "        # Predict\n",
    "        #y_pred = self.model.predict(self.generator)\n",
    "        #y_pred = np.argmax(y_pred, axis=1)\n",
    "        #print(y_pred)\n",
    "        \n",
    "        \n",
    "        #y_pred_str = []\n",
    "        #for item in y_pred:\n",
    "        #    y_pred_str.append(classes[item])\n",
    "        \n",
    "        \n",
    "        \n",
    "        steps=5\n",
    "        preds = self.model.predict(self.generator, steps=steps)\n",
    "        print(preds)\n",
    "        preds = preds.tolist()\n",
    "\n",
    "        ####\n",
    "\n",
    "        indices = []\n",
    "        for pred in preds:\n",
    "            temp = []\n",
    "            for category in pred:\n",
    "                if category>=0.3:\n",
    "                    temp.append(pred.index(category))\n",
    "            if temp!=[]:\n",
    "                indices.append(temp)\n",
    "            else:\n",
    "                temp.append(np.argmax(pred))\n",
    "                indices.append(temp)\n",
    "\n",
    "        print(indices)\n",
    "        \n",
    "\n",
    "\n",
    "        testlabels = []\n",
    "\n",
    "\n",
    "        for image in indices:\n",
    "            temp = []\n",
    "            for i in image:\n",
    "                temp.append(str(class_labels[i]))\n",
    "            testlabels.append(' '.join(temp))\n",
    "\n",
    "        print(testlabels)\n",
    "        \n",
    "        \n",
    "        self.test_files_df['labels'] = testlabels #y_pred_str\n",
    "        # Write to csv\n",
    "        self.test_files_df.to_csv('./submission.csv', index=False)\n",
    "        print(\"Submission completed: written submission.csv\")\n",
    "        return self.test_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not toy:\n",
    "    # Get dataframe for submission\n",
    "    submitter = Submitter(model, IMAGE_SIZE)\n",
    "    submission_df = submitter.predict_for_submit()     \n",
    "    submission_df.head()\n",
    "else:\n",
    "    submission_df = pd.DataFrame()\n",
    "    print(\"Do not submit in toy mode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
